# Patent RAG Prototype

A Retrieval-Augmented Generation (RAG) system for patent search and analysis using hybrid search, vector embeddings, and LLM-powered summarization.

## Project Structure & Entry Points

### 📁 Application Structure

```
patent-rag-app/
├── src/patent_rag_app/           # Main application package
│   ├── api/                      # FastAPI web application
│   │   ├── main.py              # 🚪 MAIN ENTRY POINT - FastAPI app
│   │   └── routers/             # API route handlers
│   │       ├── frontend.py      # Serves UI at "/"
│   │       ├── health.py        # Health check endpoint
│   │       ├── patents.py       # GET /patents (list with summaries)
│   │       └── search.py        # POST /search/chunks (Q&A)
│   ├── config/                  # Application configuration
│   │   ├── settings.py          # Environment variables & settings
│   │   └── logging.py           # Logging configuration
│   ├── db/                      # Database layer
│   │   ├── mongo_client.py      # MongoDB connection
│   │   ├── patent_repository.py # Patent CRUD operations
│   │   ├── chunk_repository.py  # Chunk CRUD operations
│   │   └── qdrant_client.py     # Qdrant vector DB connection
│   ├── ingestion/              # PDF parsing & data ingestion
│   │   ├── parsers/
│   │   │   └── pdf_parser.py    # PDF → structured data
│   │   ├── patent_ingestor.py   # Orchestrates parsing + summarization
│   │   ├── chunk_builder.py     # Chunks documents for search
│   │   ├── models.py            # Pydantic models for PDF data
│   │   └── schemas.py           # Database schemas
│   ├── llm/                     # AI/ML components
│   │   ├── available_patents_summarizer.py  # Llama 3.2 summarizer
│   │   ├── embeddings.py        # Text → vector embeddings
│   │   └── reranker.py          # Search result reranking
│   └── retrieval/              # Search & retrieval system
│       ├── service.py           # Main search orchestration
│       ├── qdrant_store.py      # Vector operations
│       └── model.py             # Search result models
├── scripts/                     # 🚪 CLI ENTRY POINTS
│   ├── reset_patents_collection.py     # Clear patents DB
│   ├── reset_chunks_collection.py      # Clear chunks DB
│   ├── ingest_patents.py               # PDF ingestion pipeline
│   ├── build_chunks.py                 # Chunking pipeline
│   └── index_vectors.py                # Vectorization pipeline
├── tests/                       # Test suite
├── frontend/
│   └── index.html              # Web UI (served by FastAPI)
└── data/                       # PDF files directory
```

### 🚪 Entry Points

**Web Application (Main Entry Point):**
```bash
uv run uvicorn patent_rag_app.api.main:app --reload
```
- **File**: `src/patent_rag_app/api/main.py`
- **URL**: http://127.0.0.1:8000/
- **Provides**: Web UI with patent summaries & search functionality

**Data Pipeline CLI Scripts:**
```bash
# Patent ingestion with LLM summarization
uv run python scripts/ingest_patents.py data

# Create searchable chunks
uv run python scripts/build_chunks.py

# Generate vector embeddings
uv run python scripts/index_vectors.py --recreate

# Database management
uv run python scripts/reset_patents_collection.py --force
uv run python scripts/reset_chunks_collection.py --force
```

### 🔄 Data Flow
```
PDFs (data/)
    ↓ [scripts/ingest_patents.py]
MongoDB patents (with LLM summaries)
    ↓ [scripts/build_chunks.py]
MongoDB chunks
    ↓ [scripts/index_vectors.py]
Qdrant vectors
    ↓ [api/main.py server]
Web UI + Search
```

## How the RAG System Works

### 🧠 Overview
This Patent RAG system combines traditional search with modern AI to provide intelligent patent analysis:

1. **Multi-Language Document Processing**: Extracts structured data from patent PDFs with support for English, German, and French claims sections
2. **LLM Summarization**: Uses Llama 3.2 via Ollama to generate human-readable 2-sentence summaries from patent titles and abstracts
3. **Intelligent Chunking**: Creates language-aware semantic chunks that preserve document structure and table headers
4. **Hybrid Search**: Combines lexical (BM25) + semantic (vector) search with cross-encoder reranking
5. **Context-Aware Retrieval**: Returns relevant chunks with proper context and language identification for Q&A
6. **LLM Answer Generation**: Optional AI-powered answer synthesis from retrieved chunks with grounded citations

### 🔍 Search Architecture
- **Vector Database**: Qdrant stores embeddings generated by sentence-transformers
- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2` converts text to 384-dimensional vectors
- **Reranker**: `cross-encoder/ms-marco-MiniLM-L-6-v2` reorders results for relevance
- **Chunking Strategy**: 512 tokens with 50-token overlap, preserving document sections

### 🎯 Key Features
- **Multi-Language Support**: Handles English, German (Patentansprüche), and French (Revendications) claims
- **Enhanced Patent List**: LLM-generated summaries replace garbled patent titles
- **Table Header Preservation**: Maintains column headers in search results
- **Language-Aware Chunking**: Claims labeled as "Claim 1 (German)", "Claim 1 (French)"
- **Page Header Cleanup**: Removes PDF artifacts and page continuation numbers
- **Multi-Content Search**: Searches across front matter, paragraphs, claims, and tables
- **Contextual Results**: Each result includes patent context, section type, and language
- **LLM Answer Generation**: Optional AI-powered responses with grounded citations

## Quickstart

1. Create and activate the virtual environment (Git Bash):
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
2. Install project and dev dependencies with uv:
   ```bash
   uv sync
   ```
3. Populate `.env` from your secrets (see configuration section below).
4. Install and start Ollama for patent summarization:
   ```bash
   # Download from https://ollama.ai
   ollama pull llama3.2
   ollama serve  # Runs on localhost:11434
   ```

## Configuration

Required environment variables in `.env`:

```env
# MongoDB Database
DATABASE_HOST=mongodb+srv://...
DATABASE_USER=username
DATABASE_PASSWORD=password

# Qdrant Vector Database
USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=https://...
QDRANT_APIKEY=your_api_key

# Application Settings
COLLECTION_NAME=patentv2
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# LLM Provider for Patent Summarization
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3.2
OLLAMA_URL=http://localhost:11434

# Optional: OpenAI fallback
OPENAI_MODEL_ID=gpt-4o-mini
```

## Operational Runbook

When MongoDB and Qdrant are reachable, refresh all collections and stage the data:

1. `uv run python scripts/reset_patents_collection.py --force`
   - Clears the patent metadata collection.
2. `uv run python scripts/reset_chunks_collection.py --force`
   - Drops the chunk collection.
3. `uv run python scripts/ingest_patents.py data`
   - Parses PDFs from `data/` directory and populates the patent store with front fields, paragraphs, claims, tables, and LLM-generated summaries.
4. `uv run python scripts/build_chunks.py`
   - Builds section-aware chunks (front fields, claims, token-aware description paragraphs, tables) and stores them in MongoDB.
5. `uv run python scripts/index_vectors.py --recreate`
   - Generates embeddings for the chunks and upserts them into the configured Qdrant collection (payload indexes are ensured automatically).

After these stages, serve the API locally:

```bash
uv run uvicorn patent_rag_app.api.main:app --reload
```

After (re)indexing, sanity-check claim retrieval:

```bash
uv run python tests/manual/claim_retrieval_check.py EP1577413_A1
```

## Technical Implementation

### 📊 Data Pipeline
1. **PDF Ingestion** (`scripts/ingest_patents.py`):
   - Extracts text using pdfplumber
   - Parses front matter, claims, descriptions, tables
   - Generates LLM summaries using Ollama + Llama 3.2
   - Stores structured data in MongoDB

2. **Chunk Building** (`scripts/build_chunks.py`):
   - Creates section-aware chunks (front fields, claims, paragraphs, tables)
   - Preserves table headers and document structure
   - Maintains 512-token chunks with 50-token overlap
   - Stores chunks with metadata in MongoDB

3. **Vector Indexing** (`scripts/index_vectors.py`):
   - Generates embeddings using sentence-transformers
   - Batches processing for efficiency
   - Upserts vectors to Qdrant with payload indexing
   - Recreates collections with proper vector dimensions

### 🔎 Search Flow
1. **Query Processing**: User submits natural language query via web UI
2. **Vector Search**: Query embedded and searched against Qdrant vector database
3. **Hybrid Retrieval**: Combines semantic similarity with optional patent filtering
4. **Reranking**: Cross-encoder model reorders results by relevance
5. **Result Assembly**: Returns chunks with context, headers, and patent metadata

## Frontend & API Usage

- Visit `http://127.0.0.1:8000/` in the browser to load the enhanced demo interface. The UI displays patents with LLM-generated summaries and supports natural language queries with optional AI answer generation.
- **Patent Selection**: Click on a patent in the left sidebar to select it for your questions
- **AI Answer Toggle**: Use the "🤖 Generate AI Answer" checkbox to get LLM-powered responses with citations
- Manual API calls are available via `http`/`curl`:
  ```bash
  curl -X POST http://127.0.0.1:8000/search/chunks \
    -H "Content-Type: application/json" \
    -d '{
          "query": "What are the main components?",
          "patent_id": "EP1577413_A1",
          "top_k": 5,
          "generate_answer": true
        }'
  ```
- `POST /search/chunks`: accepts `{ "query": str, "patent_id": Optional[str], "top_k": int, "generate_answer": bool }` and returns reranked chunk snippets with optional LLM-generated answers and citations.
- Use the built-in docs at `http://127.0.0.1:8000/docs` for schema details.


